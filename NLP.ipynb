{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/hamza/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/hamza/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle           # save file\n",
    "import re\n",
    "import nltk\n",
    "import pickle                           \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Awesome.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                                           Awesome.      1\n",
       "1                           Wow... Loved this place.      1\n",
       "2                                 Crust is not good.      0\n",
       "3          Not tasty and the texture was just nasty.      0\n",
       "4  Stopped by during the late May bank holiday of...      1"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the data with labels for training model \n",
    "data = pd.read_csv('https://raw.githubusercontent.com/ANKITSHARMA98/Restaurant-Review-s-Sentiment-Analyser/main/Restaurant_Reviews.tsv', delimiter='\\t', quoting=3)\n",
    "data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to pre-process data using NLP techniques\n",
    "def NLP_Process(dt):\n",
    "    corpus = []\n",
    "    for i in range (len(dt)):\n",
    "        # Remove specical characters from each review text\n",
    "        each_review = re.sub(pattern='[^a-zA-Z]', repl=\" \", string=dt[\"Review\"][i])\n",
    "\n",
    "        # Convert each review to lower case\n",
    "        each_review = each_review.lower()\n",
    "\n",
    "        # tokenize each-review text\n",
    "        review_words = each_review.split()\n",
    "\n",
    "        #Removing the stop words\n",
    "        review_words = [word for word in review_words if not word in set(stopwords.words('english'))]\n",
    "\n",
    "        ##print(review_words)\n",
    "\n",
    "        #Lemmatizing review_words\n",
    "        Lm = WordNetLemmatizer()\n",
    "        each_review = [Lm.lemmatize(word) for word in review_words]\n",
    "\n",
    "        #joinning the lemmatized words\n",
    "        each_review = \" \".join(each_review)\n",
    "\n",
    "        #create a corpus\n",
    "        corpus.append(each_review)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data corpus with pre-processing of review\n",
    "corp_one = NLP_Process(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Bag of words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "CV = CountVectorizer()\n",
    "X = CV.fit_transform(corp_one).toarray()\n",
    "y = data[\"Liked\"]\n",
    "\n",
    "# Creating a pickle file for the CountVectorizer for future use\n",
    "pickle.dump(CV, open('cv-transform.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Building\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB(alpha=0.2)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7611940298507462"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Naive Bayes Score\n",
    "classifier.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hamza/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LRM = LogisticRegression(C=1.5)\n",
    "LRM.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7860696517412935"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic regression Score\n",
    "LRM.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.5, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting Support Vector Regressor\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel=\"linear\", C=1.5)\n",
    "svc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7810945273631841"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Support Vector Regressor Score\n",
    "svc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump all models using pickle\n",
    "pickle.dump(classifier, open('review-sentiment-MNB-model.pkl', 'wb'))\n",
    "pickle.dump(classifier, open('review-sentiment-LR-model.pkl', 'wb'))\n",
    "pickle.dump(classifier, open('review-sentiment-SVC-model.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer_Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>William</td>\n",
       "      <td>03/04/2022</td>\n",
       "      <td>Super duper tasty!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bart</td>\n",
       "      <td>31/03/2022</td>\n",
       "      <td>This is the first time I’ve ordered from this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xin</td>\n",
       "      <td>29/03/2022</td>\n",
       "      <td>The food wasn't bad but didn't taste as good a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sagar</td>\n",
       "      <td>26/03/2022</td>\n",
       "      <td>I ordered 4 dishes totalling £24 , didn't even...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bee</td>\n",
       "      <td>20/03/2022</td>\n",
       "      <td>First time ordering, great food and really aut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Customer_Name        Date                                             Review\n",
       "0       William  03/04/2022                                 Super duper tasty!\n",
       "1          Bart  31/03/2022  This is the first time I’ve ordered from this ...\n",
       "2           Xin  29/03/2022  The food wasn't bad but didn't taste as good a...\n",
       "3         sagar  26/03/2022  I ordered 4 dishes totalling £24 , didn't even...\n",
       "4           Bee  20/03/2022  First time ordering, great food and really aut..."
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#real-time restaurant data obtained through Just Eat web scraping \n",
    "#trained model will be used on this data to perform predictions\n",
    "df = pd.read_csv(\"Review.csv\")\n",
    "#print(len(df))\n",
    "df = df[~(df[\"Review\"]==\"None\")]\n",
    "#print(len(df))\n",
    "df.dropna(inplace=True)\n",
    "#print(len(df))\n",
    "df = df.reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real time data from a takeaway on just eat pre-processed\n",
    "corp_two = NLP_Process(df)\n",
    "\n",
    "#vectorize real data review text\n",
    "df_vect = CV.transform(corp_two).toarray()\n",
    "\n",
    "#predict real reviews\n",
    "prediction = LRM.predict(df_vect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store predcition as a new column in df\n",
    "df[\"Prediction\"] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([125.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 248.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot predcitions\n",
    "plt.hist(df[\"Prediction\"],color=[\"green\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column with positive or negative label\n",
    "df[\"Review_Type\"] = df[\"Prediction\"].apply(lambda x: \"positive\" if x == 1 else \"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write df as csv file\n",
    "df.drop(\"Prediction\", axis=1, inplace=True)\n",
    "df.to_csv(\"Prediction.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
